<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Dual-Balancing PINN论文阅读（上） | 一个数字自留地-DIY知识库</title><meta name="author" content="FAVE"><meta name="copyright" content="FAVE"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Physics-Informed Neural Networks核心是构建一个带参数 $\theta$ 的神经网络来学习PDE的近似解 $\hat{u}_\theta (\mathbf{x}, t)$  \begin{align*} \mathcal{N}_{\mathbf{x}, t}[u(\mathbf{x}, t)] &amp;&#x3D; f(\mathbf{x}, t), \quad \mathbf{x}">
<meta property="og:type" content="article">
<meta property="og:title" content="Dual-Balancing PINN论文阅读（上）">
<meta property="og:url" content="https://srrdhy.github.io/post/20251031214414.html">
<meta property="og:site_name" content="一个数字自留地-DIY知识库">
<meta property="og:description" content="Physics-Informed Neural Networks核心是构建一个带参数 $\theta$ 的神经网络来学习PDE的近似解 $\hat{u}_\theta (\mathbf{x}, t)$  \begin{align*} \mathcal{N}_{\mathbf{x}, t}[u(\mathbf{x}, t)] &amp;&#x3D; f(\mathbf{x}, t), \quad \mathbf{x}">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://srrdhy.github.io/img/article10.jpg">
<meta property="article:published_time" content="2025-10-31T13:44:14.000Z">
<meta property="article:modified_time" content="2025-10-31T13:51:50.870Z">
<meta property="article:author" content="FAVE">
<meta property="article:tag" content="Dynamic Weighting">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://srrdhy.github.io/img/article10.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Dual-Balancing PINN论文阅读（上）",
  "url": "https://srrdhy.github.io/post/20251031214414.html",
  "image": "https://srrdhy.github.io/img/article10.jpg",
  "datePublished": "2025-10-31T13:44:14.000Z",
  "dateModified": "2025-10-31T13:51:50.870Z",
  "author": [
    {
      "@type": "Person",
      "name": "FAVE",
      "url": "https://srrdhy.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://srrdhy.github.io/post/20251031214414.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Dual-Balancing PINN论文阅读（上）',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/article10.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">一个数字自留地-DIY知识库</span></a><a class="nav-page-title" href="/"><span class="site-name">Dual-Balancing PINN论文阅读（上）</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Dual-Balancing PINN论文阅读（上）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-31T13:44:14.000Z" title="发表于 2025-10-31 21:44:14">2025-10-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T13:51:50.870Z" title="更新于 2025-10-31 21:51:50">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/PINN/">PINN</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="Physics-Informed-Neural-Networks"><a href="#Physics-Informed-Neural-Networks" class="headerlink" title="Physics-Informed Neural Networks"></a>Physics-Informed Neural Networks</h3><p>核心是构建一个带参数 $\theta$ 的神经网络来学习PDE的近似解 $\hat{u}_\theta (\mathbf{x}, t)$</p>
<script type="math/tex; mode=display">
\begin{align*} \mathcal{N}_{\mathbf{x}, t}[u(\mathbf{x}, t)] &= f(\mathbf{x}, t), \quad \mathbf{x} \in \Omega, \, t \in [0, T] \\ 
\mathcal{B}_{\mathbf{x}, t}[u(\mathbf{x}, t)] &= g(\mathbf{x}, t), \quad \mathbf{x} \in \partial\Omega, \, t \in [0, T] \\ 
u(\mathbf{x}, 0) &= h(\mathbf{x}), \quad \mathbf{x} \in \Omega \end{align*}</script><p>在时空域$(\Omega \times [0, T])$中采样点$\left{ \left( \mathbf{X}<em>i^r, t_i^r \right) \right}</em>{i=1}^{N_r}$，计算PDE残差损失：</p>
<script type="math/tex; mode=display">
\mathcal{L}^r(\theta) = \frac{1}{N_r} \sum_{i=1}^{N_r} \left\| \mathcal{N}_{\mathbf{x}, t} \left[ \hat{u}_\theta \left( \mathbf{x}_i^r, t_i^r \right) \right] - f \left( \mathbf{x}_i^r, t_i^r \right) \right\|^2</script><p>再加上一众条件损失，如边界条件，初始条件，以及额外的数据。</p>
<p>在PINN的训练中，权重系数是一个影响巨大却难以调节的数字，许多学者寻找动态权重的方法，当前主要的两种路线是the learning approach and the calculating approach。</p>
<p>learning approach视权重系数为可学习的参数，在网络中一同优化；calculating approach以neural tangent kernel (NTK)的视角，基于梯度幅值的比率$\left| \nabla<em>\theta \mathcal{L}^r \right| / \left| \nabla</em>\theta \mathcal{L}^i \right|$对权重进行计算。DB-PINN就是对现有calculating approach中的具体做法GW-PINN进行补充。</p>
<p><img src="/img/article10/001.png" alt=""></p>
<h3 id="GW-PINN"><a href="#GW-PINN" class="headerlink" title="GW-PINN"></a>GW-PINN</h3><p>旨在跨平衡(inter-balancing)，如上图中黄色边框内部所示。以往研究表明，PDE残差通常在各损失项中占主导地位，现在把残差项的权重系数固定为1，在训练过程中调整其余条件损失项的权重。</p>
<script type="math/tex; mode=display">
\mathcal{G} = \sum_{i=1}^{M} \frac{\max\left\{ \left| \nabla_\theta \mathcal{L}^r \right| \right\}}{\left| \nabla_\theta \lambda^i \mathcal{L}^i \right|}</script><p>通常总损失包括四个原子损失，残差损失$L<em>r$，边界损失$L</em>{bc}$，初始损失$L<em>{ic}$，数据损失$L</em>{d}$，</p>
<script type="math/tex; mode=display">
\mathcal{L} = L_r + \lambda_{bc} L_{bc} + \lambda_{ic} L_{ic} + \lambda_d L_d</script><p>实际训练时常出现梯度失衡：某一项对网络参数的反传梯度远大于其它项，导致别的项“学不到东西”。</p>
<p>GW-PINN要做的，就是让各类损失对网络参数的梯度量级对齐。做法是：给每个条件项一个权重</p>
<script type="math/tex; mode=display">
\lambda_i \propto \frac{s_r}{s_i}</script><p>其中 $s_r$ 是“残差项的梯度尺度”，$s_i$​ 是第 $i$ 个条件项（$bc,ic,d$）的梯度尺度。这样当某条件项梯度偏小，它的 $\lambda_i$​ 就被放大，反之缩小，实现跨类（inter）的对齐。</p>
<p>基于不同的 $s_r$​ 和 $s_i$ 算法，​GW-PINN有3种不同实现：</p>
<p>method 1，mean 版：</p>
<script type="math/tex; mode=display">
s_r = \max\left| \nabla_\theta L_r \right|, \quad s_i = \operatorname{mean}\left| \nabla_\theta L_i \right|</script><p>更新权重：</p>
<script type="math/tex; mode=display">
\lambda_i \leftarrow (1 - \alpha)\lambda_i + \alpha \cdot \frac{s_r}{s_i}</script><p>这里用了EMA(Exponential Moving Average)平滑进行权重更新，抑制瞬时噪声，比如$\alpha$ 取0.1</p>
<p>method 2，std 版：</p>
<script type="math/tex; mode=display">
s_r = std\left( \nabla_\theta L_r \right), \quad s_i = std\left( \nabla_\theta L_i \right)</script><p>同上更新</p>
<p>method 3，kurt 版：</p>
<p>先构造$\text{cov} = \text{std}/\text{kurt}$，再更新权重：</p>
<script type="math/tex; mode=display">
\lambda_i \leftarrow (1 - \alpha)\lambda_i + \alpha \cdot \frac{\text{cov}_r}{\text{cov}_i}</script><p>在代码中，以 method 1 为例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> method == <span class="number">1</span>:</span><br><span class="line">	<span class="comment"># max/avg</span></span><br><span class="line">	lamb_hat = maxr/meanb</span><br><span class="line">	lambd_bc     = (<span class="number">1</span>-alpha_ann)*lambd_bc + alpha_ann*lamb_hat </span><br><span class="line">	lamb_hat = maxr/meani</span><br><span class="line">	lambd_ic     = (<span class="number">1</span>-alpha_ann)*lambd_ic + alpha_ann*lamb_hat </span><br><span class="line">	lamb_hat = maxr/meand</span><br><span class="line">	lambd_d      = (<span class="number">1</span>-alpha_ann)*lambd_d + alpha_ann*lamb_hat </span><br></pre></td></tr></table></figure></p>
<p>要用到的 <code>max</code> <code>mean</code> <code>std</code> <code>kurt</code> 由 <code>loss_grad_stats</code> 和 <code>loss_grad_max_mean</code> 函数给出：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stdr,kurtr=loss_grad_stats(l_reg, net)   </span><br><span class="line">stdb,kurtb=loss_grad_stats(l_bc, net)</span><br><span class="line">stdi,kurti=loss_grad_stats(l_ic, net)</span><br><span class="line">stdd,kurtd=loss_grad_stats(l_data, net)</span><br><span class="line"></span><br><span class="line">maxr,meanr=loss_grad_max_mean(l_reg, net)  </span><br><span class="line">maxb,meanb=loss_grad_max_mean(l_bc, net,lambg=lambd_bc)  </span><br><span class="line">maxi,meani=loss_grad_max_mean(l_ic, net,lambg=lambd_ic)</span><br><span class="line">maxd,meand=loss_grad_max_mean(l_data, net,lambg=lambd_d)</span><br></pre></td></tr></table></figure></p>
<p>注意为了稳定，每 10 个 Adam step 更新一次权重 （LBFGS更新太慢，一般不在这里更新了）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % mm == <span class="number">0</span>:   <span class="comment"># mm = 10</span></span><br></pre></td></tr></table></figure></p>
<p>最后得到的权重带入总损失计算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = l_reg + lambd_bc.item()*l_bc + lambd_ic.item()*l_ic + lambd_d.item()*l_data</span><br></pre></td></tr></table></figure><br>注意.item() 明确切到 Python 标量，不参与计算图（不希望 λ 反传）</p>
<h3 id="max-mean-std-kurt-的算法"><a href="#max-mean-std-kurt-的算法" class="headerlink" title="max mean std kurt 的算法"></a>max mean std kurt 的算法</h3><p>以 method 1 为例，残差项的“尺度”为</p>
<script type="math/tex; mode=display">
S_r = \max_{\theta} \left| \nabla_{\theta} L_r \right|</script><p>也就是：把 $L_r$ 对“所有可训练参数 $\theta$ ”的梯度，取绝对值后的全局最大值。</p>
<p>条件项（bc/ic/data）的“尺度”为</p>
<script type="math/tex; mode=display">
S_i = \operatorname{mean}_{\theta} \left| \nabla_\theta (\lambda_i L_i) \right|</script><p>也就是：对每个条件项 $L_i$，在当前权重 $\lambda_i$ 下，对所有参数的梯度，取绝对值后的整体均值。注意这里有$\lambda_i$，对应着 <code>loss_grad_max_mean</code> 函数中的 <code>lambg</code> 参数，因为残差项 $L_r$ 权重默认为1，故无需传入 <code>lambg</code> 参数。</p>
<p><code>loss_grad_max_mean</code> 函数的具体实现如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_grad_max_mean</span>(<span class="params">loss, net, lambg=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    inputs: loss: loss function ; net: the NN model; lambg : term for weighted stats (optional)</span></span><br><span class="line"><span class="string">    outputs: max and mean</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    grad_ = torch.zeros((<span class="number">0</span>), dtype=torch.float64,device=device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历网络所有层，只看 nn.Linear 的参数（权重和偏置）</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> net.modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(m, nn.Linear): </span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(m == <span class="number">0</span>):</span><br><span class="line">            w = torch.<span class="built_in">abs</span>(lambg*grad(loss, m.weight, retain_graph=<span class="literal">True</span>, allow_unused=<span class="literal">True</span>)[<span class="number">0</span>])</span><br><span class="line">            b = torch.<span class="built_in">abs</span>(lambg*grad(loss, m.bias, retain_graph=<span class="literal">True</span>, allow_unused=<span class="literal">True</span>)[<span class="number">0</span>])        </span><br><span class="line">            grad_ = torch.cat((w.view(-<span class="number">1</span>), b))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            w = torch.<span class="built_in">abs</span>(lambg*grad(loss, m.weight, retain_graph=<span class="literal">True</span>, allow_unused=<span class="literal">True</span>)[<span class="number">0</span>]) <span class="comment"># 绝对值</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> grad(loss, m.bias, retain_graph=<span class="literal">True</span>, allow_unused=<span class="literal">True</span>)[<span class="number">0</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                b = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                b = torch.<span class="built_in">abs</span>(lambg*grad(loss, m.bias, retain_graph=<span class="literal">True</span>, allow_unused=<span class="literal">True</span>)[<span class="number">0</span>])    </span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                grad_ = torch.cat((grad_,w.view(-<span class="number">1</span>), b))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                grad_ = torch.cat((grad_,w.view(-<span class="number">1</span>)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度绝对值的最大与平均</span></span><br><span class="line">    maxgrad = torch.<span class="built_in">max</span>(grad_)</span><br><span class="line">    meangrad = torch.mean(grad_)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> maxgrad,meangrad </span><br></pre></td></tr></table></figure></p>
<p>在遍历网络所有层后，对所有参数 $p$ (m.weight，m.bias)，取$\nabla_{p}loss$：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.grad(loss, p, retain_graph=<span class="literal">True</span>, allow_unused=<span class="literal">True</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<p>乘以传入的 <code>lambg</code> 后再取绝对值，展平，拼接为超长一维向量 <code>grad_</code> ，代表该损失在当前权重下对整个网络的梯度幅值分布。</p>
<p><code>std</code> <code>kurt</code> 的取值类似在 <code>loss_grad_stats</code> 中给出。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.11117v3">[2505.11117v3] Dual-Balancing for Physics-Informed Neural Networks</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://srrdhy.github.io">FAVE</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://srrdhy.github.io/post/20251031214414.html">https://srrdhy.github.io/post/20251031214414.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://srrdhy.github.io" target="_blank">一个数字自留地-DIY知识库</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Dynamic-Weighting/">Dynamic Weighting</a></div><div class="post-share"><div class="social-share" data-image="/img/article10.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/post/20251025131418.html" title="在Windows 11上 使用 WSL 安装并运行带有图形界面的 Ubuntu 24.04"><img class="cover" src="/img/article09.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">在Windows 11上 使用 WSL 安装并运行带有图形界面的 Ubuntu 24.04</div></div><div class="info-2"><div class="info-item-1">感谢大佬的教程，此处做文字版整理和补充：  在Windows 11上 使用 WSL 安装并运行带有图形界面的 Ubuntu 24.04_哔哩哔哩_bilibili  win+R输入control，打开控制面板找到程序 选择启用或关闭Windows功能 把虚拟相关的都打开，包括Virtual Machine Platform，Windows虚拟机监控程序平台，适用于Linux的Windows子系统，然后重启电脑生效。 win+R输入cmd，打开命令行窗口，输入: 1wsl --update  更新完成后输入 1wsl -v  检查版本内核信息 如果以前安装过linux系统，可以查看已安装的 WSL 发行版 1wsl --list --verbose   删除多余的 Ubuntu 发行版，如 1wsl --unregister Ubuntu-24.04 执行后，该 Ubuntu 发行版的所有数据（文件、配置、已安装的软件等）会被彻底删除，且无法恢复。 输入 1wsl --list --online  列出可安装的linux发行版 选择所需版本下载，这里我们输入 1wsl...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/favicon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">FAVE</div><div class="author-info-description">技术博客, 记录物理学, 数学与AI的探索</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/srrdhy"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/srrdhy" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Physics-Informed-Neural-Networks"><span class="toc-number">1.</span> <span class="toc-text">Physics-Informed Neural Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GW-PINN"><span class="toc-number">2.</span> <span class="toc-text">GW-PINN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#max-mean-std-kurt-%E7%9A%84%E7%AE%97%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">max mean std kurt 的算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-number">4.</span> <span class="toc-text">Reference</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/20251031214414.html" title="Dual-Balancing PINN论文阅读（上）"><img src="/img/article10.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Dual-Balancing PINN论文阅读（上）"/></a><div class="content"><a class="title" href="/post/20251031214414.html" title="Dual-Balancing PINN论文阅读（上）">Dual-Balancing PINN论文阅读（上）</a><time datetime="2025-10-31T13:44:14.000Z" title="发表于 2025-10-31 21:44:14">2025-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/20251025131418.html" title="在Windows 11上 使用 WSL 安装并运行带有图形界面的 Ubuntu 24.04"><img src="/img/article09.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="在Windows 11上 使用 WSL 安装并运行带有图形界面的 Ubuntu 24.04"/></a><div class="content"><a class="title" href="/post/20251025131418.html" title="在Windows 11上 使用 WSL 安装并运行带有图形界面的 Ubuntu 24.04">在Windows 11上 使用 WSL 安装并运行带有图形界面的 Ubuntu 24.04</a><time datetime="2025-10-25T05:14:18.000Z" title="发表于 2025-10-25 13:14:18">2025-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/20250617195001.html" title="SDF-PINN 符号距离函数"><img src="/img/article08.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SDF-PINN 符号距离函数"/></a><div class="content"><a class="title" href="/post/20250617195001.html" title="SDF-PINN 符号距离函数">SDF-PINN 符号距离函数</a><time datetime="2025-06-17T11:50:01.000Z" title="发表于 2025-06-17 19:50:01">2025-06-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/20250617164213.html" title="FF-PINN 傅里叶特征嵌入"><img src="/img/article07.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="FF-PINN 傅里叶特征嵌入"/></a><div class="content"><a class="title" href="/post/20250617164213.html" title="FF-PINN 傅里叶特征嵌入">FF-PINN 傅里叶特征嵌入</a><time datetime="2025-06-17T08:42:13.000Z" title="发表于 2025-06-17 16:42:13">2025-06-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/20250610131629.html" title="Fourier neural operator (FNO) 解读"><img src="/img/article06.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Fourier neural operator (FNO) 解读"/></a><div class="content"><a class="title" href="/post/20250610131629.html" title="Fourier neural operator (FNO) 解读">Fourier neural operator (FNO) 解读</a><time datetime="2025-06-10T05:16:29.000Z" title="发表于 2025-06-10 13:16:29">2025-06-10</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By FAVE</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>